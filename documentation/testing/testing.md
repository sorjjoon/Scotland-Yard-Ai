# Testing

Testing has been done as unit tests, using [jest](https://jestjs.io/). Tests can be run by running 'npm run test' from the project root.

A coverage report is generated using Istanbul, which will be found in /coverage after a run. A summary will be stdout (the lastest summary can be found [here](coverage.txt))

All aspects of the program are tested, outside of client side scripts (in [/src/client/](/src/client)), which are used to implement the UI.

The bulk of the test data is example games, which are exported JSON from actual games.

Performance testing for the algorithm involved profiling the algorithm, to see if any potential performance improvments could be made. Unfortunatelly this seems impossible, as the vast majority of performance seems to be related to garbage collection, caused by having to copy to current game state to all children, when creating fresh children for the tree. [profile results](PURE_CPU_PROFILE.PNG). For example, we tried to increase the memory allocated for node.js runtime, but every increase from the default lowered performance substantially. There is no significant performance between pure MCTS and our so called "explorative MCTS".

Time complexity for the algorithm is not really applicable, since the algorithm is built around conducting playouts for a set amount of time. A single playout has a complexity of O(n), where n is the amount of moves before reaching an end state. In practice however we have noticed a massive disparity in the amount of playouts that can be completed in the allowed timeframe (usually 3-5 seconds), even for almost identical game states. Sometimes the amount of playouts can be 10 to 100 times larger for consecutive players. This is simply due to the nature of the algorithm. The algorithm relies on stumbling upon promising moves at random, if it happens to find a move leading to a quick victory, it will heavily explore that option further, dramaticly lowering the average time spent on a playout (by far the most expensive part of a playout is generating child states, which only needs to be done when in a previously unvisited state). In practice, this can be noticed when studying the debug strings shown after making a move, if the playout was heavily focused towards a single move (one move has a massively higher visits than other), the number of playouts completed tends to be high. Ig the algorithm was indecisive, and multiple promising moves were discovered (multiple moves have high visitations), the number of playouts completed tends to be smaller.

Efforts were made the profile the algorithm, to try and find ways to maximize the number of playouts in the allowed time frame. However, the most expensive part of the algorithm is generating children (see [profile results](PURE_CPU_PROFILE.PNG), "generateChildren" takes up nearly 40 % of processing time), but this is only done once per visited game state, meaning in playouts should run faster longer the simulation is allowed to run (as children are only generated once for a particular state), however form the profiling we can also see the huge majority of states we vist are never before visited (compare time spent in "rollout" to "selection"), so this will probably only matter when runing simulation for a game close to ending.

After having the Ai play against itself, it can be determined that an explorative MCTS, using sqrt(2) for exploration parameter (recommended by various scholary articles on the subject) plays significantly better than the pure MCTS. All tests were ran using 4 detectives against X, move processing time was 5 s. When PURE MCTS was playing against itself the game was heavily detecive favored, detectives managing to win around 60 % of games played (n=200). However when detectives using explorative MCTS played against an X using PURE MCTS ai, the results were 250-0 (detectives won every time). Explorative MCTS playing against itself had results similar to PURE MCTS against itself, around 70 % winrate for detectives. For future test, we've decided to drop the amount of detectives to 3, to hopefully make the game more balanced.
